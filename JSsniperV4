import re
from urllib.parse import urljoin, urlparse
import aiohttp
import asyncio
from colorama import Fore, Style

# Python Diagram
print(f" ___     _____  _____ ____     ____  ____     ___  ____") 
print(f"|    | / ___/ / ___/|    \  |    ||    \   /  _]|    \ ") 
print(f"|__  |(   \_ (   \_ |  _  |  |  | |  o  ) /  [_ |  D  ) ") 
print(f" __|  | \__  | \__  ||  |  | |  | |   _/ |    _]|    / ") 
print(f"/  |  | /  \ | /  \ ||  |  | |  | |  |   |   [_ |    \ ") 
print(f"\  `  | \    | \    ||  |  | |  | |  |   |     ||  .  \ ") 
print(f"\____j  \___|  \___||__|__||____||__|   |_____||__|\_|") 
                                   
# Signature
print("\nScript by Malek Althubiany")
print("=" * 100)

# Regex patterns
_regex = {
    'google_api': r'AIza[0-9A-Za-z-_]{35}',
    'firebase': r'AAAA[A-Za-z0-9_-]{7}:[A-Za-z0-9_-]{140}',
    'google_captcha': r'6L[0-9A-Za-z-_]{38}|^6[0-9a-zA-Z_-]{39}$',
    'google_oauth': r'ya29\.[0-9A-Za-z\-_]+',
    'amazon_aws_access_key_id': r'A[SK]IA[0-9A-Z]{16}',
    'amazon_mws_auth_token': r'amzn\\.mws\\.[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}',
    'amazon_aws_url': r's3\.amazonaws.com[/]+|[a-zA-Z0-9_-]*\.s3\.amazonaws.com',
    'amazon_aws_url2': r"(" \
                       r"[a-zA-Z0-9-\.\_]+\.s3\.amazonaws\.com" \
                       r"|s3://[a-zA-Z0-9-\.\_]+" \
                       r"|s3-[a-zA-Z0-9-\.\_\/]+" \
                       r"|s3.amazonaws.com/[a-zA-Z0-9-\.\_]+" \
                       r"|s3.console.aws.amazon.com/s3/buckets/[a-zA-Z0-9-\.\_]+)",
    'facebook_access_token': r'EAACEdEose0cBA[0-9A-Za-z]+',
    'authorization_basic': r'basic [a-zA-Z0-9=:_\+\/-]{5,100}',
    'authorization_bearer': r'bearer [a-zA-Z0-9_\-\.=:_\+\/]{5,100}',
    'authorization_api': r'api[key|_key|\s+]+[a-zA-Z0-9_\-]{5,100}',
    'mailgun_api_key': r'key-[0-9a-zA-Z]{32}',
    'twilio_api_key': r'SK[0-9a-fA-F]{32}',
    'twilio_account_sid': r'AC[a-zA-Z0-9_\-]{32}',
    'twilio_app_sid': r'AP[a-zA-Z0-9_\-]{32}',
    'paypal_braintree_access_token': r'access_token\$production\$[0-9a-z]{16}\$[0-9a-f]{32}',
    'square_oauth_secret': r'sq0csp-[ 0-9A-Za-z\-_]{43}|sq0[a-z]{3}-[0-9A-Za-z\-_]{22,43}',
    'square_access_token': r'sqOatp-[0-9A-Za-z\-_]{22}|EAAA[a-zA-Z0-9]{60}',
    'stripe_standard_api': r'sk_live_[0-9a-zA-Z]{24}',
    'stripe_restricted_api': r'rk_live_[0-9a-zA-Z]{24}',
    'github_access_token': r'[a-zA-Z0-9_-]*:[a-zA-Z0-9_\-]+@github\.com*',
    'possible_emails': r'\b(?:mailto:)?[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b|\/\*.*?@.*?\*/',
    'Saudi_phone_number': r'(\+?9665\d{8}|05\d{8})',
    'American_Phone_number':r'\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}',
    'private_ip': r'\b(?:\d{1,3}\.){3}\d{1,3}\b',
    'javascript_technologies': r'@(?:Jquery|React|Angular|Vue|Node\.js)\b|[A-Za-z]+\.version[\'"]?\s*[:=]\s*[\'"]?(\d+\.\d+\.\d+)[\'"]?',
    'api_keywords': r'\b(?:api_key|api_secret|api_token|apikey|secret_key|access_token)\b',
    'api_with_key': r'(api_key|api_secret|api_token|apikey|secret_key|access_token)=(\S+)',
    'company_url': r'(?:https?://)?(?:www\.)?([a-zA-Z0-9-]+\.[a-zA-Z]{2,}\.sa)(?:/[^\s]*)?',
    'RFC4627': r'^(?:(?:"(?:\\.|[^\\"])*")|(?:\'(?:\\.|[^\'])*\')|(?:[^\s"\',]*))\s*:\s*(?:(?:"(?:\\.|[^\\"])*")|(?:\'(?:\\.|[^\'])*\')|(?:[^\s"\',]*))\s*$',
    'RFC4627_general':r'"([^"]+)":\s*("[^"]+"|[^",]+)',
    'sql_user': r"'([^']+)'@'([^']+)'",
    'user_host_pattern' :r'(\S{1,10})@(\S+)',
    'hash':r'\b[0-9a-fA-F]{32,}\b',


     'suspicious_comments': r'(?i)\b(?:password|api[_\s]*key|secret[_\s]*key|access[_\s]*token|'
                           r'private[_\s]*key|oauth[_\s]*token)\b', 
                          'json_line': r'.*?"\w+":\s*{"\w+":\s*"[^"]+",\s*"\w+":\s*"[^"]+"}.*',                                               
                          'links':r'"[a-zA-Z0-9_]+":\s*"(https?://[^"]+)"',
                          'id': r'"id":\s*"[^"]+"',
                          'data_and_ID': r'"data":\s*{\s*"type":\s*"[^"]+",\s*"id":\s*"[^"]+"\s*}',
                          'name': r'"name":\s*"[^"]+"',
                         'password': r'password:\s*[^,}]+',
                         'username': r'username:\s*[^,}]+',
                         'userdb':r'userdb:\s*[^,}]+',
                         'Possible_password':r'password:\s*[^,}]+',
                         'possbile_sql_credentials':r'(?:\b(?:user(?:name)?|u(?:id)?|login|usr)\s*[:=]\s*[\'"]?([a-zA-Z0-9_]+)[\'"]?)\s*,?\s*(?:\b(?:password|pwd)\s*[:=]\s*[\'"]?([^\'",\s]+)[\'"]?)|(?:[a-zA-Z0-9_]+@[\w.-]+)',
                         'Possbile_sql_credntials':r'\b(?:mysql|DB_Password|DB_User)\s*:\s*([^,\s]+)\b',
                         'Possbile_sql_credntials':r'\b(?:mysql|DB_Password|DB_User)\s*:\s*([^\n,]+)',
                         'sql_query':r'\b(?:SELECT|INSERT|UPDATE|DELETE|CREATE|ALTER|DROP|GRANT|REVOKE|BEGIN|COMMIT|ROLLBACK|DECLARE|EXECUTE|USE|DESCRIBE)\b',
                         
                         'first_name_last_name':r'"firstName"\s*:\s*"(.*?)",\s*"lastName"\s*:\s*"(.*?)",\s*"twitter"\s*:\s*"(.*?)"',
                         'pssobile_extensions': r'https?://(?:www\.)?[a-zA-Z0-9-]+\.[a-zA-Z]{2,}/[^\s]*\.(php|config|html|png|js|exe|jpg|jpeg|zip|json|xml|yaml|properties|env|zip|rar)',
                         'http_request_headers':r'(?:\b(?:[A-Za-z-]+)\s*:\s*[^\r\n]+\r\n)+',
                         'http_request_methods':r'\b(GET|POST|PUT|DELETE|PATCH|OPTIONS|HEAD|CONNECT|TRACE)\b',
                         'technolgoies':r'\b(?:jQuery|React|Angular|Vue|Node\.js)\b|[A-Za-z]+\.version[\'"]?\s*[:=]\s*[\'"]?(\d+(?:\.\d+){0,2})[\'"]?',
                         
                        
                         
                         

                          
                    
}

def check_line_for_regex(text, regex_pattern, category):
    matches = re.findall(regex_pattern, text)
    if matches:
        print(f"{Fore.YELLOW}Match found ({category}): {Fore.WHITE}{matches}{Style.RESET_ALL}")

async def extract_information(url, session, error_count, success_count):
    try:
        async with session.get(url) as response:
            response_code = response.status

            if response_code == 200:
                content = await response.text()

                for category, regex_pattern in _regex.items():
                    lines = content.split('\n')
                    for line in lines:
                        check_line_for_regex(line, regex_pattern, category)

                print(f"{Fore.GREEN}[{response_code}] response code for {url}. No sensitive information found.{Style.RESET_ALL}")
                success_count.append(url)

            elif response_code == 302:
                print(f"{Fore.YELLOW}[{response_code}] response code for {url} (Redirected URL). "
                      f"Check the redirected URL.{Style.RESET_ALL}")
            elif response_code == 500:
                print(f"{Fore.BLUE}[{response_code}] response code for {url} (Internal Server Error). "
                      f"Please check the URL manually and try to bypass it.{Style.RESET_ALL}")
            else:
                error_count.append(url)
                print(f"{Fore.RED}[{response_code}] response code for {url} (Unexpected response code). "
                      f"Review and check this URL manually.{Style.RESET_ALL}")

    except Exception as e:
        error_count.append(url)
        print(f"{Fore.RED}Error processing {url}: {e}{Style.RESET_ALL}")

async def process_urls(urls, error_count, success_count):
    async with aiohttp.ClientSession(connector=aiohttp.TCPConnector(ssl=False)) as session:
        tasks = [extract_information(url, session, error_count, success_count) for url in urls]
        await asyncio.gather(*tasks)

def read_urls_from_file(file_path):
    with open(file_path, 'r') as file:
        return [line.strip() for line in file.readlines() if line.strip()]

def main():
    print(f"{Fore.BLUE}JS Sniper - Extract Information from JavaScript Files in URLs{Style.RESET_ALL}")
    print(f"{Fore.BLUE}By Malek Althubiany{Style.RESET_ALL}\n")

    option = int(input(
        f"{Fore.BLUE}Select an option:\n1. Enter a single URL\n2. Enter a file path containing multiple URLs\n{Style.RESET_ALL}"))

    error_urls = []
    success_urls = []

    if option == 1:
        url = input(f"{Fore.GREEN}Enter the URL: {Style.RESET_ALL}")
        asyncio.run(process_urls([url], error_urls, success_urls))
    elif option == 2:
        file_path = input(f"{Fore.GREEN}Enter the file path: {Style.RESET_ALL}")
        urls = read_urls_from_file(file_path)
        asyncio.run(process_urls(urls, error_urls, success_urls))

    print(f"{Fore.CYAN}{'=' * 80}")
    print("Conclusion")
    print(f"{'=' * 80}{Style.RESET_ALL}")

    error_pages = len(error_urls)
    valid_pages = len(success_urls)
    total_pages = error_pages + valid_pages

    print(f"{Fore.RED}Number of Error Pages: {error_pages}{Style.RESET_ALL}")
    print(f"{Fore.GREEN}Number of Valid Pages (200 response with No Sensitive Information): {valid_pages}{Style.RESET_ALL}")
    print(f"{Fore.YELLOW}Total Pages: {total_pages}{Style.RESET_ALL}")

    # General Recommendation
    if error_pages > 0:
        print(f"{Fore.RED}Recommendation: Some URLs encountered errors during processing. "
              f"Review and check these URLs manually.{Style.RESET_ALL}")
    elif valid_pages == 0:
        print(f"{Fore.MAGENTA}Recommendation: No sensitive information or potential secrets leaks identified in the processed URLs.{Style.RESET_ALL}")

if __name__ == "__main__":
    main()
