import re
from urllib.parse import urljoin, urlparse
import aiohttp
import asyncio
from colorama import Fore, Style



# Python Diagram
print(f" ___     _____  _____ ____     ____  ____     ___  ____") 
print(f"|    | / ___/ / ___/|    \  |    ||    \   /  _]|    \ ") 
print(f"|__  |(   \_ (   \_ |  _  |  |  | |  o  ) /  [_ |  D  ) ") 
print(f" __|  | \__  | \__  ||  |  | |  | |   _/ |    _]|    / ") 
print(f"/  |  | /  \ | /  \ ||  |  | |  | |  |   |   [_ |    \ ") 
print(f"\  `  | \    | \    ||  |  | |  | |  |   |     ||  .  \ ") 
print(f"\____j  \___|  \___||__|__||____||__|   |_____||__|\_|") 
                                   

# Signature
print("\nScript by Malek Althubiany")
print("=" * 100)

_regex = {
    'google_api': r'AIza[0-9A-Za-z-_]{35}',
    'firebase': r'AAAA[A-Za-z0-9_-]{7}:[A-Za-z0-9_-]{140}',
    'google_captcha': r'6L[0-9A-Za-z-_]{38}|^6[0-9a-zA-Z_-]{39}$',
    'google_oauth': r'ya29\.[0-9A-Za-z\-_]+',
    'amazon_aws_access_key_id': r'A[SK]IA[0-9A-Z]{16}',
    'amazon_mws_auth_token': r'amzn\\.mws\\.[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}',
    'amazon_aws_url': r's3\.amazonaws.com[/]+|[a-zA-Z0-9_-]*\.s3\.amazonaws.com',
    'amazon_aws_url2': r"(" \
                       r"[a-zA-Z0-9-\.\_]+\.s3\.amazonaws\.com" \
                       r"|s3://[a-zA-Z0-9-\.\_]+" \
                       r"|s3-[a-zA-Z0-9-\.\_\/]+" \
                       r"|s3.amazonaws.com/[a-zA-Z0-9-\.\_]+" \
                       r"|s3.console.aws.amazon.com/s3/buckets/[a-zA-Z0-9-\.\_]+)",
    'facebook_access_token': r'EAACEdEose0cBA[0-9A-Za-z]+',
    'authorization_basic': r'basic [a-zA-Z0-9=:_\+\/-]{5,100}',
    'authorization_bearer': r'bearer [a-zA-Z0-9_\-\.=:_\+\/]{5,100}',
    'authorization_api': r'api[key|_key|\s+]+[a-zA-Z0-9_\-]{5,100}',
    'mailgun_api_key': r'key-[0-9a-zA-Z]{32}',
    'twilio_api_key': r'SK[0-9a-fA-F]{32}',
    'twilio_account_sid': r'AC[a-zA-Z0-9_\-]{32}',
    'twilio_app_sid': r'AP[a-zA-Z0-9_\-]{32}',
    'paypal_braintree_access_token': r'access_token\$production\$[0-9a-z]{16}\$[0-9a-f]{32}',
    'square_oauth_secret': r'sq0csp-[ 0-9A-Za-z\-_]{43}|sq0[a-z]{3}-[0-9A-Za-z\-_]{22,43}',
    'square_access_token': r'sqOatp-[0-9A-Za-z\-_]{22}|EAAA[a-zA-Z0-9]{60}',
    'stripe_standard_api': r'sk_live_[0-9a-zA-Z]{24}',
    'stripe_restricted_api': r'rk_live_[0-9a-zA-Z]{24}',
    'github_access_token': r'[a-zA-Z0-9_-]*:[a-zA-Z0-9_\-]+@github\.com*',
    'rsa_private_key': r'-----BEGIN RSA PRIVATE KEY-----',
    'ssh_dsa_private_key': r'-----BEGIN DSA PRIVATE KEY-----',
    'ssh_dc_private_key': r'-----BEGIN EC PRIVATE KEY-----',
    'pgp_private_block': r'-----BEGIN PGP PRIVATE KEY BLOCK-----',
    'json_web_token': r'ey[A-Za-z0-9-_=]+\.[A-Za-z0-9-_=]+\.?[A-Za-z0-9-_.+/=]*$',
    'slack_token': r"\"api_token\":\"(xox[a-zA-Z]-[a-zA-Z0-9-]+)\"",
    'SSH_privKey': r"([-]+BEGIN [^\s]+ PRIVATE KEY[-]+[\s]*[^-]*[-]+END [^\s]+ PRIVATE KEY[-]+)",
    'Heroku_API_KEY': r'[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}',
    'possible_Creds': r"(?i)("
                      r"password\s*[`=:\"]+\s*[^\s]+|"
                      r"password is\s*[`=:\"]*\s*[^\s]+|"
                      r"pwd\s*[`=:\"]*\s*[^\s]+|"
                      r"passwd\s*[`=:\"]+\s*[^\s]+)",
    'php_variables_streams': r'(\$_(POST|GET|COOKIE|REQUEST|SERVER|FILES)|php://(input|stdin))',
    'subdomains': r'(?<=https?://)([a-zA-Z0-9-]+\.)+[a-zA-Z]{2,}(?=/)',
    'technologies': r'(React|Angular|Vue|jQuery|Backbone|Ember|Svelte|Django|Flask|Express|Spring|Ruby on Rails|Laravel)\.version\s*[:=]\s*[\'"]?([^\'";]+)',
}
def read_urls_from_file(file_path):
    try:
        with open(file_path, 'r') as file:
            urls = file.readlines()
            return [url.strip() for url in urls]
    except Exception as e:
        print(f"{Fore.RED}Error reading URLs from file: {e}{Style.RESET_ALL}")
        return []

def extract_subdomains(url):
    base_url = urlparse(url).netloc
    subdomains = re.findall(r'(https?://)?([a-zA-Z0-9-]+)\.' + re.escape(base_url), url)
    return [subdomain[1] for subdomain in subdomains]

async def fetch_url(session, url):
    try:
        async with session.get(url) as response:
            content = await response.read()
            return response.status, content.decode('utf-8', errors='replace')  # Return both status code and content
    except Exception as e:
        error_message = str(e)
        if "Name or service not known" in error_message:
            print(f"{Fore.RED}Error fetching {url}: Cannot connect to host. Please check the URL or your internet connection.{Style.RESET_ALL}")
        else:
            print(f"{Fore.RED}Error fetching {url}: {error_message}{Style.RESET_ALL}")
        return None, None

def validate_url(base_url, url):
    try:
        joined_url = urljoin(base_url, url)
        result = urlparse(joined_url)
        return all([result.scheme, result.netloc])
    except ValueError:
        return False

async def extract_information(url, session, error_count, success_count):
    try:
        response_code, javascript_code = await fetch_url(session, url)

        # Check HTTP response code
        if response_code == 200:
            success_count.append(url)
            print(f"{Fore.GREEN}[{response_code}] response code for {url} {Fore.MAGENTA}(No sensitive info){Style.RESET_ALL}")

            # Extract sensitive information
            credentials = re.findall(r'(?i)(password|username|credential|login)\s*[:=]\s*[\'"]?([^\'";]+)', javascript_code)
            emails = re.findall(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', javascript_code)

            # Check for sensitive secrets leaks
            sensitive_secrets = {}
            for name, regex in _regex.items():
                matches = re.findall(regex, javascript_code)
                if matches:
                    sensitive_secrets[name] = matches

            # Extract private IP addresses
            private_ips = re.findall(r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b', javascript_code)

            # Extract technologies used
            technologies = re.findall(r'(React|Angular|Vue|jQuery|Backbone|Ember|Svelte)\.version\s*[:=]\s*[\'"]?([^\'";]+)', javascript_code)

            # Extract all URLs and validate them
            all_urls = re.findall(
                r'(https?://www\.|http://www\.|https?://|http://)?[a-zA-Z0-9]{2,}\.[a-zA-Z0-9]{2,}\.[a-zA-Z0-9]{2,}(\.[a-zA-Z0-9]{2,})?'
                r'|(https?://www\.|http://www\.|https?://|http://)?[a-zA-Z]{2,}(\.[a-zA-Z]{2,})(\.[a-zA-Z]{2,})?/[a-zA-Z0-9]{2,}'
                r'|(https?://www\.|http://www\.|https?://|http://)?[a-zA-Z0-9]{2,}(\.[a-zA-Z0-9]{2,})(\.[a-zA-Z0-9]{2,})?'
                , javascript_code)
            valid_urls = [url for url in all_urls if validate_url(url, url)]

            # Extract subdomains and add them to sensitive secrets regex
            subdomains = extract_subdomains(url)
            for subdomain in subdomains:
                subdomain_regex = f'{re.escape(subdomain)}.[a-zA-Z0-9]+'
                subdomain_matches = re.findall(subdomain_regex, javascript_code)
                if subdomain_matches:
                    sensitive_secrets[subdomain] = subdomain_matches

            # Print the extracted information
            print(f"{Fore.GREEN}{'=' * 80}{Style.RESET_ALL}")
            print(f"{Fore.GREEN}Information for {url}{Style.RESET_ALL}")
            print(f"{Fore.GREEN}{'=' * 80}{Style.RESET_ALL}")

            if credentials:
                print(f"{Fore.YELLOW}Credentials: {credentials}{Style.RESET_ALL}")
            if emails:
                print(f"{Fore.YELLOW}Emails: {emails}{Style.RESET_ALL}")
            if sensitive_secrets:
                print(f"{Fore.YELLOW}Sensitive Secrets Leaks:")
                for name, matches in sensitive_secrets.items():
                    print(f"{name}: {matches}")
            if private_ips:
                print(f"{Fore.YELLOW}Private IP Addresses: {private_ips}{Style.RESET_ALL}")
            if technologies:
                print(f"{Fore.CYAN}Technologies Used: {technologies}{Style.RESET_ALL}")
            if all_urls:
                print(f"{Fore.BLUE}All URLs in JavaScript:")
                for url in all_urls:
                    print(f"  - {url}")
            if valid_urls:
                print(f"{Fore.GREEN}Validated URLs:")
                for url in valid_urls:
                    print(f"  - {url}")

            if credentials or emails or sensitive_secrets or private_ips or valid_urls:
                print(f"{Fore.MAGENTA}Recommendation: Review and secure the identified sensitive information and potential secrets leaks.{Style.RESET_ALL}")
            else:
                print(f"{Fore.MAGENTA}Recommendation: No sensitive information or potential secrets leaks identified.{Style.RESET_ALL}")
        elif response_code == 302:
            print(f"{Fore.YELLOW}[302] This page has a '302' redirection response code. Redirected URL: {url}{Style.RESET_ALL}")
        elif response_code == 500:
            print(f"{Fore.BLUE}[500] This page returns '500' code, which is an internal server error. "
                  f"Please check it manually and try to bypass it.{Style.RESET_ALL}")

    except Exception as e:
        error_count.append(url)

async def process_urls(urls):
    async with aiohttp.ClientSession() as session:
        error_count = []
        success_count = []
        tasks = [extract_information(url, session, error_count, success_count) for url in urls]
        await asyncio.gather(*tasks)
        return error_count, success_count

async def main():
    print(f"{Fore.CYAN}{'=' * 80}")
    print("JS Sniper - JavaScript Information Extractor")
    print(f"{'=' * 80}{Style.RESET_ALL}")

    option = int(input(f"{Fore.GREEN}Select an option:\n1. Enter a single URL\n2. Enter a file path containing multiple URLs\n{Style.RESET_ALL}"))

    if option == 1:
        url = input(f"{Fore.GREEN}Enter the URL: {Style.RESET_ALL}")
        error_urls, success_urls = await process_urls([url])
    elif option == 2:
        file_path = input(f"{Fore.GREEN}Enter the file path: {Style.RESET_ALL}")
        urls = read_urls_from_file(file_path)
        error_urls, success_urls = await process_urls(urls)

    print(f"{Fore.CYAN}{'=' * 80}")
    print("Conclusion")
    print(f"{'=' * 80}{Style.RESET_ALL}")

    total_pages = len(error_urls) + len(success_urls)
    total_errors = len(error_urls)
    total_successes = len(success_urls)

    print(f"{Fore.BLUE}Number of Error Pages: {total_errors}{Style.RESET_ALL}")
    print(f"{Fore.GREEN}Number of Valid Pages with No Sensitive Information: {total_successes}{Style.RESET_ALL}")
    print(f"{Fore.YELLOW}Total Pages: {total_pages}{Style.RESET_ALL}")

    # General Recommendation
    if total_errors > 0:
        print(f"{Fore.RED}Recommendation: Some URLs encountered errors during processing. "
              f"Review and check these URLs manually.{Style.RESET_ALL}")
    elif total_successes == 0:
        print(f"{Fore.MAGENTA}Recommendation: No sensitive information or potential secrets leaks identified in the processed URLs.{Style.RESET_ALL}")

if __name__ == "__main__":
    asyncio.run(main())
